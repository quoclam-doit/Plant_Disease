{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7d3c64",
   "metadata": {},
   "source": [
    "# üåø Plant Disease Classification v·ªõi MambaTSR\n",
    "\n",
    "## Google Colab Setup\n",
    "Notebook n√†y ƒë∆∞·ª£c t·ªëi ∆∞u cho Google Colab v·ªõi GPU mi·ªÖn ph√≠.\n",
    "\n",
    "**Tr∆∞·ªõc khi ch·∫°y:**\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
    "2. Upload d·ªØ li·ªáu PlantVillage ho·∫∑c mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= COLAB SETUP =============\n",
    "import sys\n",
    "\n",
    "# Check if running on Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"‚úì Running on Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive (optional)\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Clone MambaTSR repository\n",
    "    !git clone https://github.com/quoclam-doit/Plant_Disease.git\n",
    "    %cd Plant_Disease/MambaTSR\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install -q timm einops fvcore tensorboard\n",
    "    \n",
    "    # Compile selective_scan CUDA kernel\n",
    "    %cd kernels/selective_scan\n",
    "    !pip install -e .\n",
    "    %cd ../..\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Not running on Colab - make sure environment is set up correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Add MambaTSR to path\n",
    "if IN_COLAB:\n",
    "    sys.path.insert(0, '/content/Plant_Disease/MambaTSR')\n",
    "\n",
    "from models.VSSBlock_utils import Super_Mamba\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7871c3",
   "metadata": {},
   "source": [
    "## üìä Upload Dataset\n",
    "\n",
    "B·∫°n c√≥ 3 options:\n",
    "1. Upload t·ª´ m√°y local (ch·∫≠m, ~2GB)\n",
    "2. L∆∞u trong Google Drive v√† mount (khuy·∫øn ngh·ªã)\n",
    "3. Download t·ª´ link public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a572e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload from local (uncomment if needed)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# Option 2: Use from Google Drive\n",
    "if IN_COLAB:\n",
    "    DATA_ROOT = '/content/drive/MyDrive/PlantVillage'  # Adjust path\n",
    "else:\n",
    "    DATA_ROOT = r'G:\\Dataset\\Data\\PlantVillage\\PlantVillage-Dataset-master'\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "\n",
    "# Verify dataset\n",
    "if not Path(DATA_ROOT).exists():\n",
    "    print(\"‚ö†Ô∏è  Dataset not found! Please upload or adjust DATA_ROOT path\")\n",
    "else:\n",
    "    print(f\"‚úì Dataset found with {len(list(Path(DATA_ROOT).iterdir()))} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a1ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'model_name': 'Super_Mamba',\n",
    "    'dims': 3,\n",
    "    'depth': 6,\n",
    "    'num_classes': 39,\n",
    "    \n",
    "    'batch_size': 64,\n",
    "    'num_epochs': 50,  # Reduced for Colab time limits\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': True,\n",
    "    \n",
    "    'image_size': 32,\n",
    "    'brightness': 0.8,\n",
    "    'contrast': (1.0, 1.0),\n",
    "    \n",
    "    'scheduler': 'cosine',\n",
    "    'min_lr': 1e-6,\n",
    "    'patience': 15,\n",
    "    \n",
    "    'data_root': Path(DATA_ROOT),\n",
    "    'save_dir': Path('/content/models' if IN_COLAB else 'G:/Dataset/models/MambaTSR'),\n",
    "}\n",
    "\n",
    "CONFIG['save_dir'].mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\n‚úì Models will be saved to: {CONFIG['save_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.ColorJitter(brightness=CONFIG['brightness']),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "full_dataset = datasets.ImageFolder(root=CONFIG['data_root'])\n",
    "class_names = full_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"‚úì Found {num_classes} classes\")\n",
    "print(f\"‚úì Total images: {len(full_dataset):,}\")\n",
    "\n",
    "# Split dataset\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.72 * total_size)\n",
    "val_size = int(0.18 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_dataset.dataset.transform = transform_train\n",
    "val_dataset.dataset.transform = transform_val\n",
    "test_dataset.dataset.transform = transform_val\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], \n",
    "                          shuffle=True, num_workers=CONFIG['num_workers'],\n",
    "                          pin_memory=CONFIG['pin_memory'])\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'],\n",
    "                        shuffle=False, num_workers=CONFIG['num_workers'],\n",
    "                        pin_memory=CONFIG['pin_memory'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'],\n",
    "                         shuffle=False, num_workers=CONFIG['num_workers'],\n",
    "                         pin_memory=CONFIG['pin_memory'])\n",
    "\n",
    "print(f\"\\nüìä Dataset split:\")\n",
    "print(f\"  Train: {len(train_dataset):,} ({len(train_dataset)/total_size*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_dataset):,} ({len(val_dataset)/total_size*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_dataset):,} ({len(test_dataset)/total_size*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa2200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Super_Mamba(\n",
    "    dims=CONFIG['dims'],\n",
    "    depth=CONFIG['depth'],\n",
    "    num_classes=CONFIG['num_classes']\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úì Model: {CONFIG['model_name']}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "print(\"Testing forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(2, 3, CONFIG['image_size'], CONFIG['image_size']).to(device)\n",
    "    test_output = model(test_input)\n",
    "    print(f\"‚úì Input shape: {test_input.shape}\")\n",
    "    print(f\"‚úì Output shape: {test_output.shape}\")\n",
    "    print(f\"‚úì Model is ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457bb95",
   "metadata": {},
   "source": [
    "## üöÄ Training\n",
    "\n",
    "**Note:** Training s·∫Ω m·∫•t ~2-4 gi·ªù tr√™n Colab T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], \n",
    "                  weight_decay=CONFIG['weight_decay'])\n",
    "\n",
    "if CONFIG['scheduler'] == 'cosine':\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=CONFIG['num_epochs'], \n",
    "                                  eta_min=CONFIG['min_lr'])\n",
    "else:\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, \n",
    "                                  patience=5, verbose=True)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84740789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'acc': f\"{100.*train_correct/train_total:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    train_loss /= len(train_dataset)\n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_dataset)\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    \n",
    "    # Update scheduler\n",
    "    if CONFIG['scheduler'] == 'cosine':\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    print(f\"  LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'config': CONFIG\n",
    "        }, CONFIG['save_dir'] / 'super_mamba_best.pth')\n",
    "        print(f\"  ‚úì Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= CONFIG['patience']:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "print(\"\\n‚úì Training completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4833e",
   "metadata": {},
   "source": [
    "## üìà Results & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bddeb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training & Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['save_dir'] / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Training curves saved to {CONFIG['save_dir'] / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e208c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set\n",
    "checkpoint = torch.load(CONFIG['save_dir'] / 'super_mamba_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(labels.numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "print(f\"\\nüéØ Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(test_labels, test_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c45122",
   "metadata": {},
   "source": [
    "## üíæ Download Model\n",
    "\n",
    "ƒê·ªÉ download model v·ªÅ m√°y local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d5dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Download best model\n",
    "    model_path = str(CONFIG['save_dir'] / 'super_mamba_best.pth')\n",
    "    files.download(model_path)\n",
    "    print(f\"‚úì Downloaded: {model_path}\")\n",
    "    \n",
    "    # Download training curves\n",
    "    curve_path = str(CONFIG['save_dir'] / 'training_curves.png')\n",
    "    files.download(curve_path)\n",
    "    print(f\"‚úì Downloaded: {curve_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
