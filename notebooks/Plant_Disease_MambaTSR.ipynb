{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef0ae80",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è IMPORTANT: Setup Requirements\n",
    "\n",
    "## üõ†Ô∏è Prerequisites Before Running This Notebook\n",
    "\n",
    "### ‚úÖ Already Installed:\n",
    "- ‚úÖ PyTorch 2.6.0 with CUDA 12.4\n",
    "- ‚úÖ Virtual environment (`.venv`)\n",
    "- ‚úÖ MambaTSR repository cloned\n",
    "\n",
    "### ‚è≥ Still Need to Install:\n",
    "\n",
    "#### 1. Microsoft Visual C++ Build Tools (Required!)\n",
    "**Why needed:** To compile CUDA kernels for selective scan operation\n",
    "\n",
    "**Download:** https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "\n",
    "**Install workloads:**\n",
    "- ‚úÖ Desktop development with C++\n",
    "- ‚úÖ MSVC v143 - VS 2022 C++ x64/x86 build tools\n",
    "- ‚úÖ Windows 10 SDK\n",
    "\n",
    "**Time needed:** ~30-40 minutes (download + install + compile)\n",
    "\n",
    "#### 2. Compile selective_scan kernel\n",
    "After installing Build Tools, run in terminal:\n",
    "\n",
    "```powershell\n",
    "cd G:\\Dataset\\MambaTSR\\kernels\\selective_scan\n",
    "pip install --no-build-isolation -e .\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Full Setup Guide\n",
    "\n",
    "**See:** `G:\\Dataset\\MAMBATSR_SETUP_GUIDE.md` for detailed instructions\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Quick Start (After Setup)\n",
    "\n",
    "Once Build Tools is installed and selective_scan is compiled:\n",
    "1. Restart VS Code\n",
    "2. Select kernel: `.venv` (Python 3.11)\n",
    "3. Run all cells below\n",
    "\n",
    "---\n",
    "\n",
    "**Current Status:** ‚è≥ Waiting for Build Tools installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fb0ab",
   "metadata": {},
   "source": [
    "# MambaTSR for PlantVillage Disease Classification\n",
    "## √Åp d·ª•ng ki·∫øn tr√∫c MambaTSR (State Space Model) ƒë·ªÉ nh·∫≠n d·∫°ng b·ªánh c√¢y\n",
    "\n",
    "**Model**: Super_Mamba v·ªõi Vision State Space (VSS) Blocks  \n",
    "**Dataset**: PlantVillage (39 classes)  \n",
    "**Paper**: \"MambaTSR: You Only Need 90k Parameters for Traffic Sign Recognition\" (Neurocomputing, JCR Q1)  \n",
    "\n",
    "### Ki·∫øn tr√∫c ch√≠nh:\n",
    "- **ConvNet**: Embedding ban ƒë·∫ßu t·ª´ ·∫£nh RGB\n",
    "- **PatchMerging2D + VSSBlock**: 6 t·∫ßng (depth=6)\n",
    "- **SS2D (Selective Scan 2D)**: Tr√°i tim c·ªßa Mamba architecture\n",
    "- **Classifier**: LayerNorm ‚Üí AvgPool ‚Üí Linear(num_classes=39)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34970a8",
   "metadata": {},
   "source": [
    "## 1. Setup Environment & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8999a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "GPU Memory: 15.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Dataset\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5060 Ti with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5060 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import logging\n",
    "from functools import partial\n",
    "from typing import Optional, Callable, Any\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set environment variables for RTX 5060 Ti (sm_120) compatibility with PyTorch 2.6.0\n",
    "# Force CUDA to use PTX JIT compilation for unsupported compute capability\n",
    "os.environ['CUDA_FORCE_PTX_JIT'] = '1'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "# Import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.ops import Permute\n",
    "\n",
    "# Import timm\n",
    "import timm\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "# Import other utilities\n",
    "from einops import rearrange, repeat\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7120ba4b",
   "metadata": {},
   "source": [
    "## 2. Add MambaTSR to Python Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b81e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Added G:\\Dataset\\MambaTSR to sys.path\n",
      "\n",
      "Python search paths:\n",
      "  - G:\\Dataset\\MambaTSR\n",
      "  - C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\python311.zip\n",
      "  - C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\DLLs\n"
     ]
    }
   ],
   "source": [
    "# Add MambaTSR models to path\n",
    "mamba_path = Path(r'G:\\Dataset\\MambaTSR')\n",
    "if str(mamba_path) not in sys.path:\n",
    "    sys.path.insert(0, str(mamba_path))\n",
    "    print(f\"‚úì Added {mamba_path} to sys.path\")\n",
    "\n",
    "# Verify path\n",
    "print(\"\\nPython search paths:\")\n",
    "for p in sys.path[:3]:\n",
    "    print(f\"  - {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1a4335",
   "metadata": {},
   "source": [
    "## 3. Import MambaTSR Components\n",
    "\n",
    "**Quan tr·ªçng**: ƒê√¢y l√† ph·∫ßn th·∫ßy y√™u c·∫ßu - copy c√°c components t·ª´ MambaTSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15bc349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Successfully imported MambaTSR components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Dataset\\MambaTSR\\models\\vmamba.py:122: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "G:\\Dataset\\MambaTSR\\models\\vmamba.py:155: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n"
     ]
    }
   ],
   "source": [
    "# Import MambaTSR core components\n",
    "try:\n",
    "    from models.ConvNet import ConvNet\n",
    "    from models.VSSBlock import VSSBlock\n",
    "    from models.vmamba import SS2D, Mlp\n",
    "    print(\"‚úì Successfully imported MambaTSR components\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing MambaTSR components: {e}\")\n",
    "    print(\"\\nƒê·∫£m b·∫£o ƒë√£ c√†i ƒë·∫∑t selective_scan CUDA kernel:\")\n",
    "    print(\"cd G:\\\\Dataset\\\\MambaTSR\\\\kernels\\\\selective_scan\")\n",
    "    print(\"pip install .\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e141cdb",
   "metadata": {},
   "source": [
    "## 4. Define PatchMerging2D\n",
    "\n",
    "Component t·ª´ VSSBlock_utils.py - gi·∫£m spatial dimension v√† tƒÉng channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb091a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PatchMerging2D defined\n"
     ]
    }
   ],
   "source": [
    "class PatchMerging2D(nn.Module):\n",
    "    \"\"\"Patch Merging Layer - gi·∫£m H, W xu·ªëng 1/2 v√† tƒÉng channels\"\"\"\n",
    "    def __init__(self, dim, out_dim=-1, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, (2 * dim) if out_dim < 0 else out_dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def _patch_merging_pad(x: torch.Tensor):\n",
    "        H, W, _ = x.shape[-3:]\n",
    "        if (W % 2 != 0) or (H % 2 != 0):\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "        x0 = x[..., 0::2, 0::2, :]  # ... H/2 W/2 C\n",
    "        x1 = x[..., 1::2, 0::2, :]  # ... H/2 W/2 C\n",
    "        x2 = x[..., 0::2, 1::2, :]  # ... H/2 W/2 C\n",
    "        x3 = x[..., 1::2, 1::2, :]  # ... H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # ... H/2 W/2 4*C\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._patch_merging_pad(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "print(\"‚úì PatchMerging2D defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bc123",
   "metadata": {},
   "source": [
    "## 5. Define Super_Mamba Model\n",
    "\n",
    "**ƒê√¢y l√† d√≤ng 59 m√† th·∫ßy y√™u c·∫ßu** - Class Super_Mamba t·ª´ VSSBlock_utils.py  \n",
    "**Ch·ªânh s·ª≠a**: `num_classes=39` cho PlantVillage (thay v√¨ 43 cho traffic signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec3b70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Super_Mamba model defined v·ªõi num_classes=39 cho PlantVillage\n"
     ]
    }
   ],
   "source": [
    "class PermuteLayer(nn.Module):\n",
    "    \"\"\"Permute layer for dimension reordering\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x.permute(*self.args)\n",
    "\n",
    "\n",
    "class Super_Mamba(nn.Module):\n",
    "    \"\"\"Super_Mamba Model cho PlantVillage Classification\n",
    "    \n",
    "    Ki·∫øn tr√∫c:\n",
    "    - ConvNet: Feature embedding ban ƒë·∫ßu\n",
    "    - 6 stages: PatchMerging2D + VSSBlock (Selective Scan 2D)\n",
    "    - Classifier: LayerNorm ‚Üí Permute ‚Üí AvgPool ‚Üí Linear\n",
    "    \n",
    "    Args:\n",
    "        dims: S·ªë channels ban ƒë·∫ßu (default=3 cho RGB)\n",
    "        depth: S·ªë t·∫ßng VSSBlock (default=6)\n",
    "        num_classes: S·ªë classes output (39 cho PlantVillage)\n",
    "    \"\"\"\n",
    "    def __init__(self, dims=3, depth=6, num_classes=39):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.preembd = ConvNet()  # Embedding layer\n",
    "        \n",
    "        # Calculate dimensions for each layer\n",
    "        if isinstance(dims, int):\n",
    "            dims = [int(dims * 2 ** i_layer) for i_layer in range(self.depth+1)]\n",
    "        self.num_features = dims[-1]\n",
    "        self.dims = dims\n",
    "        \n",
    "        # Build layers: PatchMerging + VSSBlock repeated depth times\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.depth):\n",
    "            downsample = PatchMerging2D(\n",
    "                self.dims[i_layer],\n",
    "                self.dims[i_layer + 1],\n",
    "                norm_layer=nn.LayerNorm,\n",
    "            )\n",
    "            vss_block = VSSBlock(hidden_dim=self.dims[i_layer+1])\n",
    "            self.layers.append(downsample)\n",
    "            self.layers.append(vss_block)\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(OrderedDict(\n",
    "            norm=nn.LayerNorm(self.num_features),  # B,H,W,C\n",
    "            permute=PermuteLayer(0, 3, 1, 2),\n",
    "            avgpool=nn.AdaptiveAvgPool2d(1),\n",
    "            flatten=nn.Flatten(1),\n",
    "            head=nn.Linear(self.num_features, num_classes),\n",
    "        ))\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m: nn.Module):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.preembd(x)  # ConvNet embedding\n",
    "        x = x.permute(0, 2, 3, 1)  # [B, C, H, W] -> [B, H, W, C]\n",
    "        for layers in self.layers:\n",
    "            x = layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "print(\"‚úì Super_Mamba model defined v·ªõi num_classes=39 cho PlantVillage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3cad1",
   "metadata": {},
   "source": [
    "## 6. Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "008ecb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "  - VRAM: 15.9 GB\n",
      "  - Compute Capability: 12.0\n",
      "Using device: cuda\n",
      "\n",
      "‚úì Models will be saved to: G:\\Dataset\\models\\MambaTSR\n",
      "\n",
      "üìã Configuration:\n",
      "  model_name: Super_Mamba\n",
      "  dims: 3\n",
      "  depth: 6\n",
      "  num_classes: 39\n",
      "  batch_size: 64\n",
      "  num_epochs: 100\n",
      "  learning_rate: 0.001\n",
      "  weight_decay: 0.0001\n",
      "  num_workers: 0\n",
      "  pin_memory: True\n",
      "  image_size: 32\n",
      "  brightness: 0.8\n",
      "  contrast: (1.0, 1.0)\n",
      "  scheduler: cosine\n",
      "  min_lr: 1e-06\n",
      "  patience: 15\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úì Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  - VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"  - Compute Capability: {'.'.join(map(str, torch.cuda.get_device_capability(0)))}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è  CUDA not available, using CPU\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'Super_Mamba',\n",
    "    'dims': 3,  # Initial channels\n",
    "    'depth': 6,  # Number of VSSBlock stages\n",
    "    'num_classes': 39,  # PlantVillage classes\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 64 if torch.cuda.is_available() else 32,  # 64 for GPU, 32 for CPU\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'num_workers': 0,  # Windows compatibility\n",
    "    'pin_memory': torch.cuda.is_available(),  # True for GPU, False for CPU\n",
    "    \n",
    "    # Data augmentation\n",
    "    'image_size': 32,  # MambaTSR s·ª≠ d·ª•ng 32x32\n",
    "    'brightness': 0.8,\n",
    "    'contrast': (1.0, 1.0),\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    'scheduler': 'cosine',  # 'cosine' or 'plateau'\n",
    "    'min_lr': 1e-6,\n",
    "    \n",
    "    # Early stopping\n",
    "    'patience': 15,\n",
    "    \n",
    "    # Paths\n",
    "    'data_root': Path(r'G:\\Dataset\\Data\\PlantVillage\\PlantVillage-Dataset-master'),\n",
    "    'save_dir': Path(r'G:\\Dataset\\models\\MambaTSR'),\n",
    "}\n",
    "\n",
    "# Create save directory\n",
    "CONFIG['save_dir'].mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\n‚úì Models will be saved to: {CONFIG['save_dir']}\")\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    if not isinstance(value, Path):\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1e131",
   "metadata": {},
   "source": [
    "## 7. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ca903a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data transforms defined\n",
      "  - Train: ColorJitter + Flip + Rotation + Normalize\n",
      "  - Val/Test: Resize + Normalize\n"
     ]
    }
   ],
   "source": [
    "# Data transforms - theo c·∫•u tr√∫c c·ªßa MambaTSR\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.ColorJitter(brightness=CONFIG['brightness'], contrast=CONFIG['contrast']),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "print(\"‚úì Data transforms defined\")\n",
    "print(f\"  - Train: ColorJitter + Flip + Rotation + Normalize\")\n",
    "print(f\"  - Val/Test: Resize + Normalize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd096e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PlantVillage dataset...\n",
      "‚úì Found 39 classes: ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy']...\n",
      "‚úì Total images: 54,305\n",
      "\n",
      "üìä Dataset split:\n",
      "  - Train: 39,099 images (72.0%)\n",
      "  - Val:   9,774 images (18.0%)\n",
      "  - Test:  5,432 images (10.0%)\n",
      "‚úì Found 39 classes: ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy']...\n",
      "‚úì Total images: 54,305\n",
      "\n",
      "üìä Dataset split:\n",
      "  - Train: 39,099 images (72.0%)\n",
      "  - Val:   9,774 images (18.0%)\n",
      "  - Test:  5,432 images (10.0%)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading PlantVillage dataset...\")\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=CONFIG['data_root'])\n",
    "class_names = full_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"‚úì Found {num_classes} classes: {class_names[:5]}...\")\n",
    "print(f\"‚úì Total images: {len(full_dataset):,}\")\n",
    "\n",
    "# Split dataset: 72% train, 18% val, 10% test\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.72 * total_size)\n",
    "val_size = int(0.18 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Apply transforms\n",
    "train_dataset.dataset.transform = transform_train\n",
    "val_dataset.dataset.transform = transform_val\n",
    "test_dataset.dataset.transform = transform_val\n",
    "\n",
    "print(f\"\\nüìä Dataset split:\")\n",
    "print(f\"  - Train: {len(train_dataset):,} images ({len(train_dataset)/total_size*100:.1f}%)\")\n",
    "print(f\"  - Val:   {len(val_dataset):,} images ({len(val_dataset)/total_size*100:.1f}%)\")\n",
    "print(f\"  - Test:  {len(test_dataset):,} images ({len(test_dataset)/total_size*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c8d95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì DataLoaders created\n",
      "  - Train batches: 611\n",
      "  - Val batches: 153\n",
      "  - Test batches: 85\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=CONFIG['pin_memory']\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=CONFIG['pin_memory']\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=CONFIG['pin_memory']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì DataLoaders created\")\n",
    "print(f\"  - Train batches: {len(train_loader)}\")\n",
    "print(f\"  - Val batches: {len(val_loader)}\")\n",
    "print(f\"  - Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6dabaa",
   "metadata": {},
   "source": [
    "## 8. DataLoader Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e7e0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader...\n",
      "‚úì Batch shape: torch.Size([64, 3, 32, 32])\n",
      "‚úì Labels shape: torch.Size([64])\n",
      "‚úì Image range: [-1.964, 2.448]\n",
      "‚úì Device: cpu\n",
      "‚úì Successfully moved batch to GPU: cuda:0\n",
      "‚úì Batch shape: torch.Size([64, 3, 32, 32])\n",
      "‚úì Labels shape: torch.Size([64])\n",
      "‚úì Image range: [-1.964, 2.448]\n",
      "‚úì Device: cpu\n",
      "‚úì Successfully moved batch to GPU: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_18780\\4103824297.py:12: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  images_gpu = images.to(device)\n"
     ]
    }
   ],
   "source": [
    "# Test DataLoader\n",
    "print(\"Testing DataLoader...\")\n",
    "test_batch = next(iter(train_loader))\n",
    "images, labels = test_batch\n",
    "\n",
    "print(f\"‚úì Batch shape: {images.shape}\")\n",
    "print(f\"‚úì Labels shape: {labels.shape}\")\n",
    "print(f\"‚úì Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"‚úì Device: {images.device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    images_gpu = images.to(device)\n",
    "    print(f\"‚úì Successfully moved batch to GPU: {images_gpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57597462",
   "metadata": {},
   "source": [
    "## 9. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac008abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Super_Mamba model...\n",
      "‚úì Model created and moved to cuda\n",
      "\n",
      "üìä Model Statistics:\n",
      "  - Total parameters: 1,333,636\n",
      "  - Trainable parameters: 1,333,636\n",
      "  - Model size: 5.09 MB (float32)\n"
     ]
    }
   ],
   "source": [
    "# Create Super_Mamba model\n",
    "print(\"Initializing Super_Mamba model...\")\n",
    "model = Super_Mamba(\n",
    "    dims=CONFIG['dims'],\n",
    "    depth=CONFIG['depth'],\n",
    "    num_classes=CONFIG['num_classes']\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úì Model created and moved to {device}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "print(f\"  - Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  - Model size: {total_params * 4 / 1024**2:.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "589cf34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing forward pass...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      5\u001b[39m     test_input = torch.randn(\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, CONFIG[\u001b[33m'\u001b[39m\u001b[33mimage_size\u001b[39m\u001b[33m'\u001b[39m], CONFIG[\u001b[33m'\u001b[39m\u001b[33mimage_size\u001b[39m\u001b[33m'\u001b[39m]).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     test_output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_input.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Output shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_output.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\Dataset\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\Dataset\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mSuper_Mamba.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreembd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ConvNet embedding\u001b[39;00m\n\u001b[32m     70\u001b[39m     x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [B, C, H, W] -> [B, H, W, C]\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layers \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\Dataset\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\Dataset\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\Dataset\\MambaTSR\\models\\ConvNet.py:102\u001b[39m, in \u001b[36mConvNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     chaneel_1_max_pool = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmaxpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     desired_size = (x.size(\u001b[32m2\u001b[39m), x.size(\u001b[32m3\u001b[39m))\n\u001b[32m    104\u001b[39m     channel_1_max_pool_out = F.interpolate(chaneel_1_max_pool, size=desired_size, mode=\u001b[33m'\u001b[39m\u001b[33mbilinear\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    105\u001b[39m                                            align_corners=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\Dataset\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\Dataset\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\Dataset\\MambaTSR\\models\\ConvNet.py:24\u001b[39m, in \u001b[36mMaxPoolingChannel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\Dataset\\.venv\\Lib\\site-packages\\torch\\_jit_internal.py:624\u001b[39m, in \u001b[36mboolean_dispatch.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(*args, **kwargs)\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\Dataset\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:830\u001b[39m, in \u001b[36m_max_pool2d\u001b[39m\u001b[34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    829\u001b[39m     stride = torch.jit.annotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "print(\"\\nTesting forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(2, 3, CONFIG['image_size'], CONFIG['image_size']).to(device)\n",
    "    test_output = model(test_input)\n",
    "    print(f\"‚úì Input shape: {test_input.shape}\")\n",
    "    print(f\"‚úì Output shape: {test_output.shape}\")\n",
    "    print(f\"‚úì Output range: [{test_output.min():.3f}, {test_output.max():.3f}]\")\n",
    "\n",
    "model.train()\n",
    "print(\"‚úì Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8414f",
   "metadata": {},
   "source": [
    "## 10. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ebe445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer - theo paper MambaTSR\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "if CONFIG['scheduler'] == 'cosine':\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=CONFIG['num_epochs'],\n",
    "        eta_min=CONFIG['min_lr']\n",
    "    )\n",
    "    print(\"‚úì Using CosineAnnealingLR scheduler\")\n",
    "else:\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=CONFIG['min_lr']\n",
    "    )\n",
    "    print(\"‚úì Using ReduceLROnPlateau scheduler\")\n",
    "\n",
    "print(f\"‚úì Optimizer: AdamW (lr={CONFIG['learning_rate']}, weight_decay={CONFIG['weight_decay']})\")\n",
    "print(f\"‚úì Loss function: CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5435ba2c",
   "metadata": {},
   "source": [
    "## 11. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{running_loss/(batch_idx+1):.4f}\",\n",
    "            'acc': f\"{100.*correct/total:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, epoch, phase='Val'):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [{phase}]\")\n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{running_loss/(batch_idx+1):.4f}\",\n",
    "                'acc': f\"{100.*correct/total:.2f}%\"\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"‚úì Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca93952",
   "metadata": {},
   "source": [
    "## 12. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bf603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "# Best model tracking\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üöÄ Starting Training: Super_Mamba on PlantVillage\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Epochs: {CONFIG['num_epochs']} | Batch size: {CONFIG['batch_size']} | LR: {CONFIG['learning_rate']}\")\n",
    "print(f\"Device: {device} | Early stopping patience: {CONFIG['patience']}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, epoch\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(\n",
    "        model, val_loader, criterion, device, epoch, phase='Val'\n",
    "    )\n",
    "    \n",
    "    # Update learning rate\n",
    "    if CONFIG['scheduler'] == 'cosine':\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        scheduler.step(val_acc)\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    print(f\"  LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'config': CONFIG\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, CONFIG['save_dir'] / 'super_mamba_best.pth')\n",
    "        print(f\"  ‚úì New best model saved! Val Acc: {val_acc:.2f}%\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement ({patience_counter}/{CONFIG['patience']})\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= CONFIG['patience']:\n",
    "        print(f\"\\n‚ö†Ô∏è Early stopping triggered after {epoch+1} epochs\")\n",
    "        print(f\"Best Val Acc: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "        break\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Training complete\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì Training Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total time: {elapsed_time/3600:.2f} hours\")\n",
    "print(f\"Best Val Acc: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "print(f\"Model saved to: {CONFIG['save_dir'] / 'super_mamba_best.pth'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18d0e5e",
   "metadata": {},
   "source": [
    "## 13. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_checkpoint = {\n",
    "    'epoch': epoch + 1,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'val_acc': val_acc,\n",
    "    'val_loss': val_loss,\n",
    "    'history': history,\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "torch.save(final_checkpoint, CONFIG['save_dir'] / 'super_mamba_final.pth')\n",
    "print(f\"‚úì Final model saved to: {CONFIG['save_dir'] / 'super_mamba_final.pth'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74562ac9",
   "metadata": {},
   "source": [
    "## 14. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe939f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Acc', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val Acc', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training & Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "axes[2].plot(history['lr'], linewidth=2, color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('Learning Rate Schedule')\n",
    "axes[2].set_yscale('log')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['save_dir'] / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Training curves saved to: {CONFIG['save_dir'] / 'training_history.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082f87a",
   "metadata": {},
   "source": [
    "## 15. Load Best Model & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f640f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model for testing...\")\n",
    "checkpoint = torch.load(CONFIG['save_dir'] / 'super_mamba_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"‚úì Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"  Val Acc: {checkpoint['val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß™ Testing on Test Set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'acc': f\"{100.*correct/total:.2f}%\"\n",
    "        })\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = 100. * correct / total\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä Test Results:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Correct: {correct:,} / {total:,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6267ae74",
   "metadata": {},
   "source": [
    "## 16. Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc389dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\nüìã Classification Report:\\n\")\n",
    "print(classification_report(\n",
    "    all_labels, \n",
    "    all_preds, \n",
    "    target_names=class_names,\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20948f",
   "metadata": {},
   "source": [
    "## 17. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df54c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix - Super_Mamba on PlantVillage', fontsize=16, pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['save_dir'] / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Confusion matrix saved to: {CONFIG['save_dir'] / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d9755",
   "metadata": {},
   "source": [
    "## 18. Save Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4aa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'model_name': 'Super_Mamba',\n",
    "    'dataset': 'PlantVillage',\n",
    "    'num_classes': CONFIG['num_classes'],\n",
    "    'total_params': total_params,\n",
    "    'best_epoch': best_epoch,\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'test_loss': test_loss,\n",
    "    'training_time_hours': elapsed_time / 3600,\n",
    "    'config': {k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()}\n",
    "}\n",
    "\n",
    "with open(CONFIG['save_dir'] / 'results_summary.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(f\"‚úì Results saved to: {CONFIG['save_dir'] / 'results_summary.json'}\")\n",
    "print(\"\\n‚úÖ All done! MambaTSR training completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
