# ğŸ¯ HÆ°á»›ng dáº«n Train MambaTSR - PlantVillage

## âœ… Status: Sáº´N SÃ€NG TRAIN!

MambaTSR Ä‘Ã£ hoáº¡t Ä‘á»™ng hoÃ n háº£o trÃªn **RTX 5060 Ti (sm_120)**! ğŸ‰

---

## ğŸš€ Báº¯t Ä‘áº§u Training

### CÃ¡ch 1: Script tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)

```bash
wsl bash /mnt/g/Dataset/start_training.sh
```

### CÃ¡ch 2: Python trá»±c tiáº¿p

```bash
wsl bash -c "/mnt/g/Dataset/.venv_wsl/bin/python /mnt/g/Dataset/train_mambatsr_plantvillage.py"
```

### CÃ¡ch 3: Test pipeline trÆ°á»›c (an toÃ n hÆ¡n)

```bash
wsl bash -c "/mnt/g/Dataset/.venv_wsl/bin/python /mnt/g/Dataset/test_train_pipeline.py"
```

---

## ğŸ“Š ThÃ´ng sá»‘ Training

### Dataset PlantVillage

- **54,304 áº£nh** tá»« 38 loáº¡i bá»‡nh cÃ¢y trá»“ng
- **Train**: 43,440 áº£nh (80%)
- **Validation**: 10,860 áº£nh (20%)

### Model: VSSM-Tiny

- **Parameters**: 3M
- **Architecture**: [2, 2, 9, 2] layers
- **Channels**: [96, 192, 384, 768]

### Hyperparameters

- **Batch size**: 32 (tá»‘i Æ°u cho RTX 5060 Ti)
- **Epochs**: 50
- **Learning rate**: 1e-4 (AdamW + Cosine scheduler)
- **Augmentation**: Flip, rotate, color jitter

---

## â±ï¸ Thá»i gian Training

- **1 epoch**: ~10-15 phÃºt
- **50 epochs**: ~8-12 giá»
- CÃ³ thá»ƒ Ä‘á»ƒ cháº¡y qua Ä‘Ãªm! ğŸŒ™

---

## ğŸ’¾ Checkpoints & Results

LÆ°u táº¡i: `G:\Dataset\models\MambaTSR\`

```
â”œâ”€â”€ mambatsr_best.pth          # Model tá»‘t nháº¥t
â”œâ”€â”€ mambatsr_epoch_5.pth       # Checkpoint má»—i 5 epochs
â”œâ”€â”€ training_history.json      # Metrics: loss, accuracy
â””â”€â”€ class_names.json           # 38 disease classes
```

---

## ğŸ”§ TÃ¹y chá»‰nh (Optional)

Edit `train_mambatsr_plantvillage.py`:

```python
class MambaTSRConfig:
    batch_size = 32      # Giáº£m náº¿u out of memory
    num_epochs = 50      # TÄƒng/giáº£m epochs
    learning_rate = 1e-4 # Adjust learning rate
```

---

## ğŸ’¡ GPU Memory

**RTX 5060 Ti 16GB**:

- Training sá»­ dá»¥ng: ~6-8 GB
- CÃ²n trá»‘ng: ~8-10 GB
- âœ… Ráº¥t Ä‘á»§ cho batch size 32!

Náº¿u **Out of Memory**: giáº£m `batch_size = 16`

---

## ğŸ“ˆ Monitoring

Training sáº½ hiá»ƒn thá»‹:

```
Epoch 10/50 [Train]: 100%|â–ˆâ–ˆâ–ˆ| 1357/1357 [10:23<00:00, 10.01it/s]
  loss=1.234, acc=67.89%, lr=0.000095

Validation - Loss: 1.123, Accuracy: 72.34%

Epoch 10/50 Summary:
  Train - Loss: 1.234, Acc: 67.89%
  Val   - Loss: 1.123, Acc: 72.34%
  âœ“ New best validation accuracy: 72.34%
  Checkpoint saved!
```

---

## ğŸ¯ Sau khi Train xong

### Load model Ä‘á»ƒ inference:

```python
import torch

# Load best model
checkpoint = torch.load('models/MambaTSR/mambatsr_best.pth')

# Create model
from MambaTSR.models.vmamba import VSSM
model = VSSM(**checkpoint['config'])
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Inference
output = model(input_image)
```

---

## âš¡ Performance

**RTX 5060 Ti (sm_120)** - Tested & Working! âœ…

- Forward pass: 116 MB/batch
- Training speed: 10-15 it/s
- GPU utilization: 90-100%
- Memory efficient: 6-8 GB peak

---

## ğŸ› ï¸ Troubleshooting

### Out of Memory?

â†’ Giáº£m `batch_size = 16` hoáº·c `8`

### Training cháº­m?

â†’ Check GPU usage: `wsl nvidia-smi`
â†’ TÄƒng `num_workers` náº¿u CPU idle

### Accuracy khÃ´ng tÄƒng?

â†’ Giáº£m learning rate: `1e-5`
â†’ TÄƒng epochs: `100`
â†’ Check data quality

---

## âœ… Checklist

TrÆ°á»›c khi train, Ä‘áº£m báº£o:

- [x] GPU hoáº¡t Ä‘á»™ng (RTX 5060 Ti detected)
- [x] CUDA available
- [x] selective_scan compiled (sm_90 forward compatible)
- [x] Dataset loaded (54,304 images)
- [x] Model builds (3M parameters)
- [x] Forward pass works âœ…
- [ ] **Ready to train!** ğŸš€

---

## ğŸ“š Technical Details

Chi tiáº¿t Ä‘áº§y Ä‘á»§: `MAMBATSR_RTX5060TI_FINAL_STATUS.md`

**Environment:**

- PyTorch: 2.10.0.dev (nightly, cu128)
- CUDA: 12.4
- GPU: RTX 5060 Ti 16GB (sm_120)
- selective_scan: 97 MB (3 extensions)

---

**Sáºµn sÃ ng train rá»“i! Báº¯t Ä‘áº§u thÃ´i! ğŸš€ğŸŒ±**

```bash
wsl bash /mnt/g/Dataset/start_training.sh
```

---

_Last updated: November 10, 2025_
