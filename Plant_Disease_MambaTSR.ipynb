{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef0ae80",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è IMPORTANT: Setup Requirements\n",
    "\n",
    "## üõ†Ô∏è Prerequisites Before Running This Notebook\n",
    "\n",
    "### ‚úÖ Already Installed:\n",
    "- ‚úÖ PyTorch 2.6.0 with CUDA 12.4\n",
    "- ‚úÖ Virtual environment (`.venv`)\n",
    "- ‚úÖ MambaTSR repository cloned\n",
    "\n",
    "### ‚è≥ Still Need to Install:\n",
    "\n",
    "#### 1. Microsoft Visual C++ Build Tools (Required!)\n",
    "**Why needed:** To compile CUDA kernels for selective scan operation\n",
    "\n",
    "**Download:** https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "\n",
    "**Install workloads:**\n",
    "- ‚úÖ Desktop development with C++\n",
    "- ‚úÖ MSVC v143 - VS 2022 C++ x64/x86 build tools\n",
    "- ‚úÖ Windows 10 SDK\n",
    "\n",
    "**Time needed:** ~30-40 minutes (download + install + compile)\n",
    "\n",
    "#### 2. Compile selective_scan kernel\n",
    "After installing Build Tools, run in terminal:\n",
    "\n",
    "```powershell\n",
    "cd G:\\Dataset\\MambaTSR\\kernels\\selective_scan\n",
    "pip install --no-build-isolation -e .\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Full Setup Guide\n",
    "\n",
    "**See:** `G:\\Dataset\\MAMBATSR_SETUP_GUIDE.md` for detailed instructions\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Quick Start (After Setup)\n",
    "\n",
    "Once Build Tools is installed and selective_scan is compiled:\n",
    "1. Restart VS Code\n",
    "2. Select kernel: `.venv` (Python 3.11)\n",
    "3. Run all cells below\n",
    "\n",
    "---\n",
    "\n",
    "**Current Status:** ‚è≥ Waiting for Build Tools installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fb0ab",
   "metadata": {},
   "source": [
    "# MambaTSR for PlantVillage Disease Classification\n",
    "## √Åp d·ª•ng ki·∫øn tr√∫c MambaTSR (State Space Model) ƒë·ªÉ nh·∫≠n d·∫°ng b·ªánh c√¢y\n",
    "\n",
    "**Model**: Super_Mamba v·ªõi Vision State Space (VSS) Blocks  \n",
    "**Dataset**: PlantVillage (39 classes)  \n",
    "**Paper**: \"MambaTSR: You Only Need 90k Parameters for Traffic Sign Recognition\" (Neurocomputing, JCR Q1)  \n",
    "\n",
    "### Ki·∫øn tr√∫c ch√≠nh:\n",
    "- **ConvNet**: Embedding ban ƒë·∫ßu t·ª´ ·∫£nh RGB\n",
    "- **PatchMerging2D + VSSBlock**: 6 t·∫ßng (depth=6)\n",
    "- **SS2D (Selective Scan 2D)**: Tr√°i tim c·ªßa Mamba architecture\n",
    "- **Classifier**: LayerNorm ‚Üí AvgPool ‚Üí Linear(num_classes=39)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34970a8",
   "metadata": {},
   "source": [
    "## 1. Setup Environment & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8999a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import logging\n",
    "from functools import partial\n",
    "from typing import Optional, Callable, Any\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "# Import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.ops import Permute\n",
    "\n",
    "# Import timm\n",
    "import timm\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "# Import other utilities\n",
    "from einops import rearrange, repeat\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7120ba4b",
   "metadata": {},
   "source": [
    "## 2. Add MambaTSR to Python Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b81e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add MambaTSR models to path\n",
    "mamba_path = Path(r'G:\\Dataset\\MambaTSR')\n",
    "if str(mamba_path) not in sys.path:\n",
    "    sys.path.insert(0, str(mamba_path))\n",
    "    print(f\"‚úì Added {mamba_path} to sys.path\")\n",
    "\n",
    "# Verify path\n",
    "print(\"\\nPython search paths:\")\n",
    "for p in sys.path[:3]:\n",
    "    print(f\"  - {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1a4335",
   "metadata": {},
   "source": [
    "## 3. Import MambaTSR Components\n",
    "\n",
    "**Quan tr·ªçng**: ƒê√¢y l√† ph·∫ßn th·∫ßy y√™u c·∫ßu - copy c√°c components t·ª´ MambaTSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15bc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MambaTSR core components\n",
    "try:\n",
    "    from models.ConvNet import ConvNet\n",
    "    from models.VSSBlock import VSSBlock\n",
    "    from models.vmamba import SS2D, Mlp\n",
    "    print(\"‚úì Successfully imported MambaTSR components\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing MambaTSR components: {e}\")\n",
    "    print(\"\\nƒê·∫£m b·∫£o ƒë√£ c√†i ƒë·∫∑t selective_scan CUDA kernel:\")\n",
    "    print(\"cd G:\\\\Dataset\\\\MambaTSR\\\\kernels\\\\selective_scan\")\n",
    "    print(\"pip install .\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e141cdb",
   "metadata": {},
   "source": [
    "## 4. Define PatchMerging2D\n",
    "\n",
    "Component t·ª´ VSSBlock_utils.py - gi·∫£m spatial dimension v√† tƒÉng channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb091a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging2D(nn.Module):\n",
    "    \"\"\"Patch Merging Layer - gi·∫£m H, W xu·ªëng 1/2 v√† tƒÉng channels\"\"\"\n",
    "    def __init__(self, dim, out_dim=-1, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, (2 * dim) if out_dim < 0 else out_dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def _patch_merging_pad(x: torch.Tensor):\n",
    "        H, W, _ = x.shape[-3:]\n",
    "        if (W % 2 != 0) or (H % 2 != 0):\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "        x0 = x[..., 0::2, 0::2, :]  # ... H/2 W/2 C\n",
    "        x1 = x[..., 1::2, 0::2, :]  # ... H/2 W/2 C\n",
    "        x2 = x[..., 0::2, 1::2, :]  # ... H/2 W/2 C\n",
    "        x3 = x[..., 1::2, 1::2, :]  # ... H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # ... H/2 W/2 4*C\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._patch_merging_pad(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "print(\"‚úì PatchMerging2D defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bc123",
   "metadata": {},
   "source": [
    "## 5. Define Super_Mamba Model\n",
    "\n",
    "**ƒê√¢y l√† d√≤ng 59 m√† th·∫ßy y√™u c·∫ßu** - Class Super_Mamba t·ª´ VSSBlock_utils.py  \n",
    "**Ch·ªânh s·ª≠a**: `num_classes=39` cho PlantVillage (thay v√¨ 43 cho traffic signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermuteLayer(nn.Module):\n",
    "    \"\"\"Permute layer for dimension reordering\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x.permute(*self.args)\n",
    "\n",
    "\n",
    "class Super_Mamba(nn.Module):\n",
    "    \"\"\"Super_Mamba Model cho PlantVillage Classification\n",
    "    \n",
    "    Ki·∫øn tr√∫c:\n",
    "    - ConvNet: Feature embedding ban ƒë·∫ßu\n",
    "    - 6 stages: PatchMerging2D + VSSBlock (Selective Scan 2D)\n",
    "    - Classifier: LayerNorm ‚Üí Permute ‚Üí AvgPool ‚Üí Linear\n",
    "    \n",
    "    Args:\n",
    "        dims: S·ªë channels ban ƒë·∫ßu (default=3 cho RGB)\n",
    "        depth: S·ªë t·∫ßng VSSBlock (default=6)\n",
    "        num_classes: S·ªë classes output (39 cho PlantVillage)\n",
    "    \"\"\"\n",
    "    def __init__(self, dims=3, depth=6, num_classes=39):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.preembd = ConvNet()  # Embedding layer\n",
    "        \n",
    "        # Calculate dimensions for each layer\n",
    "        if isinstance(dims, int):\n",
    "            dims = [int(dims * 2 ** i_layer) for i_layer in range(self.depth+1)]\n",
    "        self.num_features = dims[-1]\n",
    "        self.dims = dims\n",
    "        \n",
    "        # Build layers: PatchMerging + VSSBlock repeated depth times\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.depth):\n",
    "            downsample = PatchMerging2D(\n",
    "                self.dims[i_layer],\n",
    "                self.dims[i_layer + 1],\n",
    "                norm_layer=nn.LayerNorm,\n",
    "            )\n",
    "            vss_block = VSSBlock(hidden_dim=self.dims[i_layer+1])\n",
    "            self.layers.append(downsample)\n",
    "            self.layers.append(vss_block)\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(OrderedDict(\n",
    "            norm=nn.LayerNorm(self.num_features),  # B,H,W,C\n",
    "            permute=PermuteLayer(0, 3, 1, 2),\n",
    "            avgpool=nn.AdaptiveAvgPool2d(1),\n",
    "            flatten=nn.Flatten(1),\n",
    "            head=nn.Linear(self.num_features, num_classes),\n",
    "        ))\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m: nn.Module):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.preembd(x)  # ConvNet embedding\n",
    "        x = x.permute(0, 2, 3, 1)  # [B, C, H, W] -> [B, H, W, C]\n",
    "        for layers in self.layers:\n",
    "            x = layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "print(\"‚úì Super_Mamba model defined v·ªõi num_classes=39 cho PlantVillage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3cad1",
   "metadata": {},
   "source": [
    "## 6. Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ecb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'Super_Mamba',\n",
    "    'dims': 3,  # Initial channels\n",
    "    'depth': 6,  # Number of VSSBlock stages\n",
    "    'num_classes': 39,  # PlantVillage classes\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 64,\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'num_workers': 0,  # Windows compatibility\n",
    "    'pin_memory': True if torch.cuda.is_available() else False,\n",
    "    \n",
    "    # Data augmentation\n",
    "    'image_size': 32,  # MambaTSR s·ª≠ d·ª•ng 32x32\n",
    "    'brightness': 0.8,\n",
    "    'contrast': (1.0, 1.0),\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    'scheduler': 'cosine',  # 'cosine' or 'plateau'\n",
    "    'min_lr': 1e-6,\n",
    "    \n",
    "    # Early stopping\n",
    "    'patience': 15,\n",
    "    \n",
    "    # Paths\n",
    "    'data_root': Path(r'G:\\Dataset\\Data\\PlantVillage\\PlantVillage-Dataset-master'),\n",
    "    'save_dir': Path(r'G:\\Dataset\\models\\MambaTSR'),\n",
    "}\n",
    "\n",
    "# Create save directory\n",
    "CONFIG['save_dir'].mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\n‚úì Models will be saved to: {CONFIG['save_dir']}\")\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    if not isinstance(value, Path):\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1e131",
   "metadata": {},
   "source": [
    "## 7. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca903a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms - theo c·∫•u tr√∫c c·ªßa MambaTSR\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.ColorJitter(brightness=CONFIG['brightness'], contrast=CONFIG['contrast']),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "print(\"‚úì Data transforms defined\")\n",
    "print(f\"  - Train: ColorJitter + Flip + Rotation + Normalize\")\n",
    "print(f\"  - Val/Test: Resize + Normalize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd096e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading PlantVillage dataset...\")\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=CONFIG['data_root'])\n",
    "class_names = full_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"‚úì Found {num_classes} classes: {class_names[:5]}...\")\n",
    "print(f\"‚úì Total images: {len(full_dataset):,}\")\n",
    "\n",
    "# Split dataset: 72% train, 18% val, 10% test\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.72 * total_size)\n",
    "val_size = int(0.18 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Apply transforms\n",
    "train_dataset.dataset.transform = transform_train\n",
    "val_dataset.dataset.transform = transform_val\n",
    "test_dataset.dataset.transform = transform_val\n",
    "\n",
    "print(f\"\\nüìä Dataset split:\")\n",
    "print(f\"  - Train: {len(train_dataset):,} images ({len(train_dataset)/total_size*100:.1f}%)\")\n",
    "print(f\"  - Val:   {len(val_dataset):,} images ({len(val_dataset)/total_size*100:.1f}%)\")\n",
    "print(f\"  - Test:  {len(test_dataset):,} images ({len(test_dataset)/total_size*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=CONFIG['pin_memory']\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=CONFIG['pin_memory']\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=CONFIG['pin_memory']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì DataLoaders created\")\n",
    "print(f\"  - Train batches: {len(train_loader)}\")\n",
    "print(f\"  - Val batches: {len(val_loader)}\")\n",
    "print(f\"  - Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6dabaa",
   "metadata": {},
   "source": [
    "## 8. DataLoader Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataLoader\n",
    "print(\"Testing DataLoader...\")\n",
    "test_batch = next(iter(train_loader))\n",
    "images, labels = test_batch\n",
    "\n",
    "print(f\"‚úì Batch shape: {images.shape}\")\n",
    "print(f\"‚úì Labels shape: {labels.shape}\")\n",
    "print(f\"‚úì Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"‚úì Device: {images.device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    images_gpu = images.to(device)\n",
    "    print(f\"‚úì Successfully moved batch to GPU: {images_gpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57597462",
   "metadata": {},
   "source": [
    "## 9. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac008abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Super_Mamba model\n",
    "print(\"Initializing Super_Mamba model...\")\n",
    "model = Super_Mamba(\n",
    "    dims=CONFIG['dims'],\n",
    "    depth=CONFIG['depth'],\n",
    "    num_classes=CONFIG['num_classes']\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úì Model created and moved to {device}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "print(f\"  - Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  - Model size: {total_params * 4 / 1024**2:.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589cf34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "print(\"\\nTesting forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(2, 3, CONFIG['image_size'], CONFIG['image_size']).to(device)\n",
    "    test_output = model(test_input)\n",
    "    print(f\"‚úì Input shape: {test_input.shape}\")\n",
    "    print(f\"‚úì Output shape: {test_output.shape}\")\n",
    "    print(f\"‚úì Output range: [{test_output.min():.3f}, {test_output.max():.3f}]\")\n",
    "\n",
    "model.train()\n",
    "print(\"‚úì Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8414f",
   "metadata": {},
   "source": [
    "## 10. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ebe445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer - theo paper MambaTSR\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "if CONFIG['scheduler'] == 'cosine':\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=CONFIG['num_epochs'],\n",
    "        eta_min=CONFIG['min_lr']\n",
    "    )\n",
    "    print(\"‚úì Using CosineAnnealingLR scheduler\")\n",
    "else:\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=CONFIG['min_lr']\n",
    "    )\n",
    "    print(\"‚úì Using ReduceLROnPlateau scheduler\")\n",
    "\n",
    "print(f\"‚úì Optimizer: AdamW (lr={CONFIG['learning_rate']}, weight_decay={CONFIG['weight_decay']})\")\n",
    "print(f\"‚úì Loss function: CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5435ba2c",
   "metadata": {},
   "source": [
    "## 11. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{running_loss/(batch_idx+1):.4f}\",\n",
    "            'acc': f\"{100.*correct/total:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, epoch, phase='Val'):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [{phase}]\")\n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{running_loss/(batch_idx+1):.4f}\",\n",
    "                'acc': f\"{100.*correct/total:.2f}%\"\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"‚úì Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca93952",
   "metadata": {},
   "source": [
    "## 12. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bf603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "# Best model tracking\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üöÄ Starting Training: Super_Mamba on PlantVillage\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Epochs: {CONFIG['num_epochs']} | Batch size: {CONFIG['batch_size']} | LR: {CONFIG['learning_rate']}\")\n",
    "print(f\"Device: {device} | Early stopping patience: {CONFIG['patience']}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, epoch\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(\n",
    "        model, val_loader, criterion, device, epoch, phase='Val'\n",
    "    )\n",
    "    \n",
    "    # Update learning rate\n",
    "    if CONFIG['scheduler'] == 'cosine':\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        scheduler.step(val_acc)\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    print(f\"  LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'config': CONFIG\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, CONFIG['save_dir'] / 'super_mamba_best.pth')\n",
    "        print(f\"  ‚úì New best model saved! Val Acc: {val_acc:.2f}%\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement ({patience_counter}/{CONFIG['patience']})\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= CONFIG['patience']:\n",
    "        print(f\"\\n‚ö†Ô∏è Early stopping triggered after {epoch+1} epochs\")\n",
    "        print(f\"Best Val Acc: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "        break\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Training complete\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì Training Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total time: {elapsed_time/3600:.2f} hours\")\n",
    "print(f\"Best Val Acc: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "print(f\"Model saved to: {CONFIG['save_dir'] / 'super_mamba_best.pth'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18d0e5e",
   "metadata": {},
   "source": [
    "## 13. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_checkpoint = {\n",
    "    'epoch': epoch + 1,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'val_acc': val_acc,\n",
    "    'val_loss': val_loss,\n",
    "    'history': history,\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "torch.save(final_checkpoint, CONFIG['save_dir'] / 'super_mamba_final.pth')\n",
    "print(f\"‚úì Final model saved to: {CONFIG['save_dir'] / 'super_mamba_final.pth'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74562ac9",
   "metadata": {},
   "source": [
    "## 14. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe939f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Acc', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val Acc', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training & Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "axes[2].plot(history['lr'], linewidth=2, color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('Learning Rate Schedule')\n",
    "axes[2].set_yscale('log')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['save_dir'] / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Training curves saved to: {CONFIG['save_dir'] / 'training_history.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082f87a",
   "metadata": {},
   "source": [
    "## 15. Load Best Model & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f640f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model for testing...\")\n",
    "checkpoint = torch.load(CONFIG['save_dir'] / 'super_mamba_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"‚úì Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"  Val Acc: {checkpoint['val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß™ Testing on Test Set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'acc': f\"{100.*correct/total:.2f}%\"\n",
    "        })\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = 100. * correct / total\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä Test Results:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Correct: {correct:,} / {total:,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6267ae74",
   "metadata": {},
   "source": [
    "## 16. Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc389dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\nüìã Classification Report:\\n\")\n",
    "print(classification_report(\n",
    "    all_labels, \n",
    "    all_preds, \n",
    "    target_names=class_names,\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20948f",
   "metadata": {},
   "source": [
    "## 17. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df54c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix - Super_Mamba on PlantVillage', fontsize=16, pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['save_dir'] / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Confusion matrix saved to: {CONFIG['save_dir'] / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d9755",
   "metadata": {},
   "source": [
    "## 18. Save Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4aa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'model_name': 'Super_Mamba',\n",
    "    'dataset': 'PlantVillage',\n",
    "    'num_classes': CONFIG['num_classes'],\n",
    "    'total_params': total_params,\n",
    "    'best_epoch': best_epoch,\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'test_loss': test_loss,\n",
    "    'training_time_hours': elapsed_time / 3600,\n",
    "    'config': {k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()}\n",
    "}\n",
    "\n",
    "with open(CONFIG['save_dir'] / 'results_summary.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(f\"‚úì Results saved to: {CONFIG['save_dir'] / 'results_summary.json'}\")\n",
    "print(\"\\n‚úÖ All done! MambaTSR training completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
